{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48abada9-10d8-4a6c-b72b-f5010aa6f09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33bcfee6-bf97-45d6-a657-d183fb1b2da5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://kubernetes.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x25a2ae87820>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02264f8e-be92-4449-a82d-f72d0310843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('README.md', 'r')\n",
    "data = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ddf5cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 文字列のカウント\n",
    "data.count(\"\\n\")\n",
    "moziretu = ('hogehoge1')\n",
    "moziretu.count('1'),moziretu.count('hoge'),moziretu.count('g1'),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1179258",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Apache Spark\n",
      "\n",
      "\n",
      "\n",
      "Spark is a unified analytics engine for large-scale data processing. It provides\n",
      "\n",
      "# Apache Spark\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f = open('README.md', 'r')\n",
    "\n",
    "datalist = f.readlines()\n",
    "print (datalist[0])\n",
    "print (datalist[1])\n",
    "print (datalist[2])\n",
    "\n",
    "f.close()\n",
    "print(datalist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "006e995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72493d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "111a0f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  4|5.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df)\n",
    "# All DataFrames above result same.\n",
    "df.show()\n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad363b64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94ef4704",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f = open('README.md', 'r')\n",
    "data = f.read()\n",
    "f.close()\n",
    "lines = sc.textFile('README.md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6438538d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blue', 1), ('green', 1), ('red', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pyspark.RDD.map\n",
    "rdd = sc.parallelize(['red','blue', 'green'], 3)\n",
    "sorted(rdd.map(lambda x: (x, 1)).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0ea9690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pyspark.RDD.sample\n",
    "rdd = sc.parallelize(range(100), 4)\n",
    "6 <= rdd.sample(False, fraction=0.1, seed=81).count() <= 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c22ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a56f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.replace('\\n', ''))\n",
    "tmp = data.replace('\\n', '').split(' ')\n",
    "tmpdict={}\n",
    "tmp\n",
    "for ele in tmp:\n",
    "    if ele not in tmpdict.keys():\n",
    "        tmpdict[ele] = 1\n",
    "    else :\n",
    "        tmpdict[ele] += 1\n",
    "tmpdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa3abda1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unbound method dict.values() needs an argument",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21968/1425434676.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mvalues_sorted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unbound method dict.values() needs an argument"
     ]
    }
   ],
   "source": [
    "values_sorted = sorted(dict.values())\n",
    "print(values_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5293ac2-3b4e-4ce4-b8a6-a108cfd35992",
   "metadata": {},
   "source": [
    "##### ※細かい処理の内容が気になる方は詳解Apach sparkを購入してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b6a25d-5752-427d-a99a-eea61e63a436",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 変換\n",
    "##### RDDに処理を加えて新しいRDDを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2362e65e-05a6-4794-a749-b483a1590321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.SparkContext.parallelize\n",
    "# コレクションからのパーティション数5でRDDの作成\n",
    "nums = sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4107f1a8-cfeb-4f48-8c68-bea7647e0e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [2], [3], [4], [6]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "331f2351-c1da-4cef-baa9-974924709d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (2, 3), (4, 5), (6, 7), (8, 9)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zip(RDD,RDD) -> RDD\n",
    "array1 = sc.parallelize([0, 2, 4, 6, 8], 5)\n",
    "array2 = sc.parallelize([1, 3, 5, 7, 9], 5)\n",
    "\n",
    "# RDDs have the same number of partitions and the same number of elements in each partition\n",
    "array1.zip(array2)\n",
    "array1.zip(array2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43255dbe-fd01-4b9a-b13a-37e40e50f247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 3, 1, 1, 2, 3]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 和集合\n",
    "# パーティションの結合は行われない。パーティション間でデータをやり取りする必要があるかが判断基準になりそう\n",
    "rdd = sc.parallelize([1, 1, 2, 3])\n",
    "rdd.union(rdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2e2398-7957-4cbd-8d37-3553b60de7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD.subtract(other, numPartitions=None)\n",
    "# otherに含まれないselfを返す\n",
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4), (\"b\", 5), (\"a\", 3)])\n",
    "y = sc.parallelize([(\"a\", 3), (\"c\", None)])\n",
    "sorted(x.subtract(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95fe9de5-7c7b-4c2a-b682-00a62bffbd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 1), (1, 2), (2, 1), (2, 2)]\n",
      "[(1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (2, 3), (3, 1), (3, 2), (3, 3)]\n"
     ]
    }
   ],
   "source": [
    "# pyspark.RDD.cartesian (直積)\n",
    "# RDD.cartesian(other)\n",
    "rdd = sc.parallelize([1, 2])\n",
    "print(sorted(rdd.cartesian(rdd).collect()))\n",
    "rdd = sc.parallelize([1, 2, 3])\n",
    "print(sorted(rdd.cartesian(rdd).collect()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9c1f69-5654-44ad-86a9-bb192e5ce54b",
   "metadata": {},
   "source": [
    "### アクション\n",
    "##### **ワーカでの処理結果をまとめ上げ**て、通常のコレクションへの変換、外部ストレージへの値の保存をする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cf82a18-ae4c-47e7-9d1c-0b31e04282e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 3, 1, 1, 2, 3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pyspark.RDD.collect\n",
    "# パーティションのRDDをドライバプログラムにすべて返却してしまう\n",
    "rdd = sc.parallelize([1, 1, 2, 3])\n",
    "rdd.union(rdd).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e72371b-e8cf-4917-bef0-97fac9b5506e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count\n",
    "# RDDの要素数を返却\n",
    "rdd = sc.parallelize([1, 1, 2, 3])\n",
    "rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6203efc1-62a3-4b3e-9624-179be8501f84",
   "metadata": {
    "tags": []
   },
   "source": [
    "### アクション 集約"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6658311-6d7e-4d59-9b8c-399405487f37",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can not reduce() empty RDD",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_848/2134226281.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# sc.parallelize((2 for _ in range(10))).map(lambda x: 5).cache().reduce(add)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\spark-3.2.0-bin-hadoop3.2\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mreduce\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m   1000\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1002\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can not reduce() empty RDD\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1004\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtreeReduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Can not reduce() empty RDD"
     ]
    }
   ],
   "source": [
    "# RDDの要素を集約。各パーティションで集約を行い、全パーティションで集約の結果をマージ\n",
    "from operator import add\n",
    "sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n",
    "sc.parallelize((2 for _ in range(10))).map(lambda x: 5).cache().reduce(add)\n",
    "sc.parallelize([]).reduce(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2029eba7-0705-4a1b-878a-7774928b8525",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "2100\n"
     ]
    }
   ],
   "source": [
    "# RDD.fold(zeroValue, op)\n",
    "from operator import add\n",
    "print(sc.parallelize([1, 2, 3, 4, 5]).fold(0, add))\n",
    "print(sc.parallelize([],).fold(100, add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c1ad172b-efd4-453b-96e8-8b104587397a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "# パーティション内での集約時にzerovalueの値が加算される\n",
    "# スライスする線+1=パーティション数\n",
    "print(sc.parallelize([],numSlices=15).fold(100, add))\n",
    "print(sc.parallelize([],numSlices=5).fold(100, add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b99feaaa-266a-4b9b-9beb-e209c9d92a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum\n",
    "sc.parallelize([1.0, 2.0, 3.0]).sum()\n",
    "# stats()\n",
    "# -> pyspark.statcounter.StatCounter\n",
    "stats = sc.parallelize([1.0, 2.0, 3.0]).stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "870e3f8a-8bc3-4d29-bbf6-c5d8d22a8d3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 3, mean: 2.0, stdev: 0.816496580927726, max: 3.0, min: 1.0)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b31b0b-9bcb-41ce-80df-0e6df4ce2de8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ペアRDD\n",
    "- 各要素がキーとバリューのペアからなるタプルで構成されるRDD\n",
    "- キーの集約時にキーのハッシュ地に基づいて同一パーティションにデータを集めることができる\n",
    "- ユースケース：日付ごとの売り上げ等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d371984-300a-4f26-931d-7deaab139f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "map キーの変更なしでもパーティションを失う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa91ed1d-4600-43a5-aece-147eeb4b7ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 1), ('b', 1), ('c', 1), ('d', 1), ('e', 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pyspark.RDD.mapでペアRDDを作成\n",
    "rdd = sc.parallelize(['a','b','c','d','e'])\n",
    "pairrdd = rdd.map(lambda x: (x, 1))\n",
    "pairrdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9473f8c6-00bb-4919-80cc-709582ef9462",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapvalues パーティションを保つ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22013729-6561-4ad0-995c-63798b1150a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 3), ('b', 1)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", [\"apple\", \"banana\", \"lemon\"]), (\"b\", [\"grapes\"])])\n",
    "def f(x): return len(x)\n",
    "x.mapValues(f).collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36278328-f0d6-4c28-8202-bc49ab2d48aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD.mapPartitions(f, preservesPartitioning=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "034b7633-946e-4722-b1cb-e90fcf10596a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 22]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1, 2, 3, 4, 5, 6, 7], 2)\n",
    "def f(iterator): yield sum(iterator)\n",
    "rdd.mapPartitions(f,preservesPartitioning=True).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f88f9e1-abbc-4af2-ae29-e61a9aea09db",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduceは値を返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a01a6004-7309-4f33-99bf-18416c69de77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "type(sc.parallelize([1, 2, 3, 4, 5]).reduce(add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aabcefc-bbca-4c21-b8cb-a66728c12734",
   "metadata": {},
   "outputs": [],
   "source": [
    "reducebykeyはRDDを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "786753d3-3d6c-452c-84bd-c736c19f20a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import add\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "type(rdd.reduceByKey(add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3aa2f90e-7642-4f89-8f9d-277b897fe8d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 2), ('b', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(rdd.reduceByKey(add).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33097c2-fc9e-4b5e-b80c-0506f2c800c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "foldは値を返す foldbykeyはRDDを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dda38ef5-bf6e-403c-aff0-3724442bfb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(sc.parallelize([1, 2, 3, 4, 5]).fold(0, add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f4afff0-c4c8-44ac-8c7e-98cda2da06b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', 1), ('a', 2)]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "from operator import add\n",
    "print(rdd.foldByKey(0, add).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826b2c7a-9f7f-49c0-89a5-65e8a69f22d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD.groupByKeyでキーごとに値を連結\n",
    "RDD.groupByKey(numPartitions=None, partitionFunc=<function portable_hash>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd428d75-c675-4b38-8765-fbfb40eff9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "- pyspark.resultiterable.ResultIterableが返ってきてこれにlistやlenを引数として与えないとみれない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3d87008-c03e-4ddc-8d7f-beb4de26eec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', <pyspark.resultiterable.ResultIterable at 0x1912b155910>),\n",
       " ('a', <pyspark.resultiterable.ResultIterable at 0x1912b31ba30>)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "rdd.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b8ecadd-6cf6-4e8a-9b0d-f1c7eb10f483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 2), ('b', 1)]\n",
      "[('a', [1, 1]), ('b', [1])]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(rdd.groupByKey().mapValues(len).collect()))\n",
    "print(sorted(rdd.groupByKey().mapValues(list).collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceec8b7-26ed-4cc1-bc13-856e6aa95745",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD.sortByKey（ascending = True、numPartitions = None、keyfunc = <functionRDD。<lambda >> ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3931f328-d2df-43b0-9f7e-2a10d4fd550d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', 3)\n",
      "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
      "[('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5), ('little', 4), ('Mary', 1), ('was', 8), ('white', 9), ('whose', 6)]\n"
     ]
    }
   ],
   "source": [
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "print(sc.parallelize(tmp).sortByKey().first())\n",
    "print(sc.parallelize(tmp).sortByKey(True,numPartitions=1).collect())\n",
    "print(sc.parallelize(tmp).sortByKey(True,numPartitions=2).collect())\n",
    "\n",
    "tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
    "tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
    "print(sc.parallelize(tmp2).sortByKey(True,numPartitions=3, keyfunc=lambda k: k.lower()).collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40ee0860-50e1-4cbb-ba19-087f042d3d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.RDD.join\n",
    "RDD.join(other, numPartitions=None)[source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "140849b9-8f65-4a56-b61f-ddd0fca61d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', (1, 2)), ('a', (1, 3))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 4)])\n",
    "y = sc.parallelize([(\"a\", 2), (\"a\", 3)])\n",
    "sorted(x.join(y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bd7c010-4ae5-4162-8aed-9e4532fa313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD.persist（storageLevel = StorageLevel（False、True、False、False、1））[ソース]\n",
    "このRDDのストレージレベルを設定して、最初に計算された後も操作間でその値を保持します。これは、RDDにストレージレベルがまだ設定されていない場合にのみ、新しいストレージレベルを割り当てるために使用できます。ストレージレベルが指定されていない場合、デフォルトで（MEMORY_ONLY）になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11695b5a-a744-4ca7-8aa0-61cdf61533b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(True, False, False, False, 1)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "pyspark.StorageLevel.MEMORY_ONLY\n",
    "pyspark.StorageLevel.DISK_ONLY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3905dba-271d-4530-a5f4-83e5ce112d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD.persist（storageLevel = StorageLevel（False、True、False、False、1））"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8bd36d2c-7ed6-4c3c-be5a-5454280896f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[76] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "rdd = sc.parallelize([\"b\", \"a\", \"c\"])\n",
    "# rdd.persist(storageLevel=pyspark.StorageLevel.DISK_ONLY)\n",
    "rdd.persist(storageLevel=pyspark.StorageLevel(True, False, False, False, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9cf882e6-dafc-40a1-90ea-ec98bd0f5358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[77] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0f68b5bd-ad36-4f75-bcf2-4e3f812e2c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sc.accumulator(1)\n",
    "a.value\n",
    "\n",
    "a.value = 2\n",
    "a.value\n",
    "\n",
    "a += 5\n",
    "a.value\n",
    "\n",
    "sc.accumulator(1.0).value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5194cff2-fe32-4941-be81-1ceb5b702419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# sc.accumulator(3J).value\n",
    "a = sc.accumulator(10)\n",
    "a.value\n",
    "\n",
    "rdd = sc.parallelize([1,2,3])\n",
    "def f(x):\n",
    "    global a\n",
    "    a += x\n",
    "    \n",
    "rdd.foreach(f)\n",
    "a.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "eea024f7-2fd6-473e-a755-69a07dc2e079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f159d3b-988f-4c48-b04b-002a1a42a2d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "b = sc.accumulator(0)\n",
    "def g(x):\n",
    "    b.add(x)\n",
    "rdd.foreach(g)\n",
    "b.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e963231-63dc-46cb-bcde-0ad63623e570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29277d2c-011b-42c1-a90d-31b0ff054dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
