{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84038f9a-794e-4083-9732-ed67a75e7412",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = [[1, '2012/01/05', '岡野 徹'],\n",
    "[2, '2012/1/0', '庄野 弘一'],\n",
    "[3, '2012/01/3', '若山 みどり'],\n",
    "[4, '2012/1/19', '岡野 徹']]\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "schema = StructType([\n",
    "   StructField(\"id\", IntegerType(), True),\n",
    "   StructField(\"date\", StringType(), True),\n",
    "   StructField(\"name\", StringType(), True)]\n",
    ")\n",
    "df = spark.createDataFrame(tmp, schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab875063-6263-4a5b-9dcc-300d0226d33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import pyspark\n",
    "import re\n",
    "\n",
    "def get_particular_type_columns(_type: pyspark.sql.types.StringType) -> List:\n",
    "    stringcol_list = []\n",
    "    for col in range(len(df.schema)):\n",
    "        if type(df.schema[col].dataType) == _type:\n",
    "            stringcol_list.append((df.schema[col].name))\n",
    "            \n",
    "    return stringcol_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e348b1ee-ebbc-42d5-9bec-1dbbf7bbdc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'df': ['date', 'name']}\n",
      "out_record\n",
      " [Row(id=1, date='2012/01/05', name='岡野 徹'), Row(id=2, date='2012/1/0', name='庄野 弘一'), Row(id=3, date='2012/01/3', name='若山 みどり'), Row(id=4, date='2012/1/19', name='岡野 徹')]\n",
      "out_dict\n",
      " {'df': [Row(id=1, date='2012/01/05', name='岡野 徹'), Row(id=2, date='2012/1/0', name='庄野 弘一'), Row(id=3, date='2012/01/3', name='若山 みどり'), Row(id=4, date='2012/1/19', name='岡野 徹')]}\n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r'[0-9]{4}/[0-9]{1}/[0-9]{1}|[0-9]{4}/[0-9]{2}/[0-9]{2}|[0-9]{4}/[0-9]{2}/[0-9]{1}|[0-9]{4}/[0-9]{1}/[0-9]{2}')\n",
    "tablename_list = ['df']\n",
    "\n",
    "col_dict = {}\n",
    "for tablename in tablename_list:\n",
    "    col_list = get_particular_type_columns(_type=pyspark.sql.types.StringType)\n",
    "    col_dict[f'{tablename}'] = col_list\n",
    "    \n",
    "print(col_dict)\n",
    "\n",
    "out_dict = {}\n",
    "for tablename in tablename_list:\n",
    "    \n",
    "    # print(','.join(stringcol_dict[f'{table}']))\n",
    "    selected_col = ','.join(col_dict[f'{tablename}'])\n",
    "    \n",
    "    # df = spark.sql(f'''\n",
    "    # select {selected_col}\n",
    "    # from {table}\n",
    "    # limit 10\n",
    "    # ''')\n",
    "    \n",
    "    collection = df.collect()\n",
    "    \n",
    "    # 10レコードに対して正規表現マッチを行ってマッチしたレコードをリストに追加していく\n",
    "    out_record = []\n",
    "    for record in collection:\n",
    "        for value in record:\n",
    "            if type(pattern.match(str(value))) == re.Match:  # 外す\n",
    "                # print(value)\n",
    "                out_record.append(record)\n",
    "    \n",
    "    out_dict[f'{tablename}'] = out_record\n",
    "\n",
    "print('out_record\\n',out_record)    \n",
    "print('out_dict\\n',out_dict)    \n",
    "\n",
    "# print(f'table name:{tablename}')\n",
    "# print(selected_col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
