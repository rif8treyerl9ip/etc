{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e24c0ec-90be-4207-bc0f-de5bdbd9e96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1de9df-b32b-4bb4-87b2-ff83a6a4a0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectors.sparseの練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80b23d83-7140-45e9-9ab5-30bf0ded4f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0.  0. -2.]\n",
      "[  99.    0.    0. -100.]\n"
     ]
    }
   ],
   "source": [
    "print(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]).toArray())\n",
    "print(Vectors.sparse(4, [(0, 99), (3, -100)]).toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a6e41dd-03e5-4220-9afb-f7b040b57092",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),\n",
    "        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),\n",
    "        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),\n",
    "        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]\n",
    "df = spark.createDataFrame(data, [\"features\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9fd1fe7c-7a7e-43db-9239-e5b8db31f5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(4,[0,3],[1.0,-2.0])|\n",
      "|   [4.0,5.0,0.0,3.0]|\n",
      "|   [6.0,7.0,0.0,8.0]|\n",
      "| (4,[0,3],[9.0,1.0])|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d9d73e3e-2312-4273-8213-b35d0733a780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Correlation.corr(df, \"features\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "767a4990-e4f5-473e-8cc6-ea32d2449fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.Row"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = Correlation.corr(df, \"features\").head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "be48a57f-454a-46d2-9ee3-7116f3c11c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\spark-3.2.0-bin-hadoop3.2\\spark-3.2.0-bin-hadoop3.2\\python\\pyspark\\sql\\context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      "DenseMatrix([[1.        , 0.05564149,        nan, 0.40047142],\n",
      "             [0.05564149, 1.        ,        nan, 0.91359586],\n",
      "             [       nan,        nan, 1.        ,        nan],\n",
      "             [0.40047142, 0.91359586,        nan, 1.        ]])\n",
      "Spearman correlation matrix:\n",
      "DenseMatrix([[1.        , 0.10540926,        nan, 0.4       ],\n",
      "             [0.10540926, 1.        ,        nan, 0.9486833 ],\n",
      "             [       nan,        nan, 1.        ,        nan],\n",
      "             [0.4       , 0.9486833 ,        nan, 1.        ]])\n"
     ]
    }
   ],
   "source": [
    "r1 = Correlation.corr(df, \"features\").head()\n",
    "print(\"Pearson correlation matrix:\\n\" + str(r1[0]))\n",
    "\n",
    "r2 = Correlation.corr(df, \"features\", \"spearman\").head()\n",
    "print(\"Spearman correlation matrix:\\n\" + str(r2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b5cffb-bb64-449b-a656-cd986f5f1075",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "columns = [\"label\", \"features\"]\n",
    "# Prepare training data from a list of (label, features) tuples.\n",
    "training = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f9a90d1-9052-4b86-a339-11806b93b9e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n",
      " label    | 1.0            \n",
      " features | [0.0,1.1,0.1]  \n",
      "-RECORD 1------------------\n",
      " label    | 0.0            \n",
      " features | [2.0,1.0,-1.0] \n",
      "-RECORD 2------------------\n",
      " label    | 0.0            \n",
      " features | [2.0,1.3,1.0]  \n",
      "-RECORD 3------------------\n",
      " label    | 1.0            \n",
      " features | [0.0,1.2,-0.5] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "training.toPandas()\n",
    "training.show(n=20, truncate=True, vertical=False)\n",
    "training.show(n=20, truncate=True, vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1cc92f5-1134-4e36-809a-1178f9e7bc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LogisticRegression instance. This instance is an Estimator.\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "# Print out the parameters, documentation, and any default values.\n",
    "# print(\"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9037ea75-c884-4f7a-b05a-a9ff70ea00dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn a LogisticRegression model. This uses the parameters stored in lr.\n",
    "model1 = lr.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6062bd30-214b-49f1-844a-f88e9a0f9f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 was fit using parameters: \n",
      "{Param(parent='LogisticRegression_c59d6c3cdea0', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2, Param(parent='LogisticRegression_c59d6c3cdea0', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0, Param(parent='LogisticRegression_c59d6c3cdea0', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial'): 'auto', Param(parent='LogisticRegression_c59d6c3cdea0', name='featuresCol', doc='features column name.'): 'features', Param(parent='LogisticRegression_c59d6c3cdea0', name='fitIntercept', doc='whether to fit an intercept term.'): True, Param(parent='LogisticRegression_c59d6c3cdea0', name='labelCol', doc='label column name.'): 'label', Param(parent='LogisticRegression_c59d6c3cdea0', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0, Param(parent='LogisticRegression_c59d6c3cdea0', name='maxIter', doc='max number of iterations (>= 0).'): 10, Param(parent='LogisticRegression_c59d6c3cdea0', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='LogisticRegression_c59d6c3cdea0', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability', Param(parent='LogisticRegression_c59d6c3cdea0', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction', Param(parent='LogisticRegression_c59d6c3cdea0', name='regParam', doc='regularization parameter (>= 0).'): 0.01, Param(parent='LogisticRegression_c59d6c3cdea0', name='standardization', doc='whether to standardize the training features before fitting the model.'): True, Param(parent='LogisticRegression_c59d6c3cdea0', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.5, Param(parent='LogisticRegression_c59d6c3cdea0', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}\n"
     ]
    }
   ],
   "source": [
    "# Since model1 is a Model (i.e., a transformer produced by an Estimator),\n",
    "# we can view the parameters it used during fit().\n",
    "# This prints the parameter (name: value) pairs, where names are unique IDs for this\n",
    "# LogisticRegression instance.\n",
    "print(\"Model 1 was fit using parameters: \")\n",
    "print(model1.extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65795e00-2d27-4b1b-8a1b-466b437f3888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We may alternatively specify parameters using a Python dictionary as a paramMap\n",
    "paramMap = {lr.maxIter: 20}\n",
    "paramMap[lr.maxIter] = 30  # Specify 1 Param, overwriting the original maxIter.\n",
    "# Specify multiple Params.\n",
    "paramMap.update({lr.regParam: 0.1, lr.threshold: 0.55})  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46e926ac-2309-4e8f-83e0-2e26050048eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can combine paramMaps, which are python dictionaries.\n",
    "# Change output column name\n",
    "paramMap2 = {lr.probabilityCol: \"myProbability\"}  # type: ignore\n",
    "paramMapCombined = paramMap.copy()\n",
    "paramMapCombined.update(paramMap2)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3822f22c-20ae-492c-ae44-3af85383c942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   Param(parent='LogisticRegression_c59d6c3cdea0', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'myProbability',\n",
      "    Param(parent='LogisticRegression_c59d6c3cdea0', name='regParam', doc='regularization parameter (>= 0).'): 0.1,\n",
      "    Param(parent='LogisticRegression_c59d6c3cdea0', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.55,\n",
      "    Param(parent='LogisticRegression_c59d6c3cdea0', name='maxIter', doc='max number of iterations (>= 0).'): 30}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(paramMapCombined,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "413cd4df-f8f7-4ec0-8d37-16a19a980d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 was fit using parameters: \n",
      "{Param(parent='LogisticRegression_c59d6c3cdea0', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2, Param(parent='LogisticRegression_c59d6c3cdea0', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0, Param(parent='LogisticRegression_c59d6c3cdea0', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial'): 'auto', Param(parent='LogisticRegression_c59d6c3cdea0', name='featuresCol', doc='features column name.'): 'features', Param(parent='LogisticRegression_c59d6c3cdea0', name='fitIntercept', doc='whether to fit an intercept term.'): True, Param(parent='LogisticRegression_c59d6c3cdea0', name='labelCol', doc='label column name.'): 'label', Param(parent='LogisticRegression_c59d6c3cdea0', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0, Param(parent='LogisticRegression_c59d6c3cdea0', name='maxIter', doc='max number of iterations (>= 0).'): 30, Param(parent='LogisticRegression_c59d6c3cdea0', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='LogisticRegression_c59d6c3cdea0', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'myProbability', Param(parent='LogisticRegression_c59d6c3cdea0', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction', Param(parent='LogisticRegression_c59d6c3cdea0', name='regParam', doc='regularization parameter (>= 0).'): 0.1, Param(parent='LogisticRegression_c59d6c3cdea0', name='standardization', doc='whether to standardize the training features before fitting the model.'): True, Param(parent='LogisticRegression_c59d6c3cdea0', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.55, Param(parent='LogisticRegression_c59d6c3cdea0', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}\n"
     ]
    }
   ],
   "source": [
    "# Now learn a new model using the paramMapCombined parameters.\n",
    "# paramMapCombined overrides all parameters set earlier via lr.set* methods.\n",
    "model2 = lr.fit(training, paramMapCombined)\n",
    "print(\"Model 2 was fit using parameters: \")\n",
    "print(model2.extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "738675f5-2eb6-47a2-b72d-79d03c37a2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "test = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n",
    "    (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n",
    "    (1.0, Vectors.dense([0.0, 2.2, -1.5]))], [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "62ae943b-45f3-42d5-9396-936f617123f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "|label|      features|       rawPrediction|       myProbability|prediction|\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "|  1.0|[-1.0,1.5,1.3]|[-2.8046567890310...|[0.05707304993572...|       1.0|\n",
      "|  0.0|[3.0,2.0,-0.1]|[2.49587585164645...|[0.92385219564432...|       0.0|\n",
      "+-----+--------------+--------------------+--------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4696314d-a8bc-4091-b536-bc3739cc88cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features=[-1.0,1.5,1.3], label=1.0 -> prob=[0.0570730499357254,0.9429269500642746], prediction=1.0\n",
      "features=[3.0,2.0,-0.1], label=0.0 -> prob=[0.9238521956443227,0.07614780435567725], prediction=0.0\n",
      "features=[0.0,2.2,-1.5], label=1.0 -> prob=[0.10972780286187778,0.8902721971381222], prediction=1.0\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on test data using the Transformer.transform() method.\n",
    "# LogisticRegression.transform will only use the 'features' column.\n",
    "# Note that model2.transform() outputs a \"myProbability\" column instead of the usual\n",
    "# 'probability' column since we renamed the lr.probabilityCol parameter previously.\n",
    "prediction = model2.transform(test)\n",
    "result = prediction.select(\"features\", \"label\", \"myProbability\", \"prediction\") \\\n",
    "    .collect()\n",
    "\n",
    "for row in result:\n",
    "    print(\"features=%s, label=%s -> prob=%s, prediction=%s\"\n",
    "          % (row.features, row.label, row.myProbability, row.prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cc56fe-5f06-424c-af3b-11aa90a2c245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
