{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e26387-1a98-4ab3-ab95-ecb8a1cc9fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20210214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30a4d27e-d779-4e9e-871a-be561290f367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=PySparkShell>\n",
      "[('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.app.name', 'PySparkShell'), ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    }
   ],
   "source": [
    "# SparkConfクラスからインスタンスを初期化し、SparkContextクラスのインスタンス時の引数に入れる\n",
    "# pysparkだとSparkContextクラスのインスタンスが最初から初期化されている\n",
    "\n",
    "print(sc)  # 最初から初期化されている\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "conf = SparkConf()\n",
    "# conf.set('spark.executor.memory', '2g')\n",
    "print(conf.getAll())\n",
    "\n",
    "# もう存在するのでエラーにならないよう既存のものをstopする必要あり\n",
    "sc.stop()\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea20411f-34f2-47f6-98ff-063cd1459812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x1c6aa4b2d00>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5e7196a-97fb-4f93-9094-fa5897beb68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local-1644843835282\n",
      "2\n",
      "20\n",
      "{}\n",
      "1644843835148\n",
      "http://kubernetes.docker.internal:4041\n",
      "3.2.0\n"
     ]
    }
   ],
   "source": [
    "# Attributes\n",
    "print(sc.applicationId)\n",
    "print(sc.defaultMinPartitions)  # A unique identifier for the Spark application.\n",
    "print(sc.defaultParallelism)  # CPUのスレッド数だっけ？そしたら21だったと思うけど\n",
    "print(sc.resources)\n",
    "print(sc.startTime)\n",
    "print(sc.uiWebUrl)\n",
    "print(sc.version)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ebe97a-1494-4665-b99d-be77c998113a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb151e40-afb9-40cf-9135-11442aacf384",
   "metadata": {},
   "source": [
    "# sparkConfの設定をいろいろ変えてみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "949c0aa1-7a5f-4489-b4db-e04ddf1f56b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.name', 'PySparkShell'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf()\n",
    "conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "95f6c5e9-2062-4060-93cb-5c2a647b6116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.app.name', 'changed app name'),\n",
      " ('spark.submit.pyFiles', ''),\n",
      " ('spark.submit.deployMode', 'client'),\n",
      " ('spark.master', 'CHANGED'),\n",
      " ('spark.ui.showConsoleProgress', 'true')]\n",
      "[('spark.executorEnv.VAR1', 'value1'),\n",
      " ('spark.app.name', 'changed app name'),\n",
      " ('spark.submit.pyFiles', ''),\n",
      " ('spark.submit.deployMode', 'client'),\n",
      " ('spark.master', 'CHANGED'),\n",
      " ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "conf.setMaster(\"local\").setAppName(\"changed app name\")\n",
    "conf.setMaster(\"CHANGED\")\n",
    "pprint(conf.getAll())\n",
    "\n",
    "conf.setExecutorEnv(\"VAR1\", \"value1\")\n",
    "pprint(conf.getAll())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab8cd9b-da9e-4d6e-a377-a64d062fa6ac",
   "metadata": {},
   "source": [
    "# ほかのアプローチ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb20f19-a290-4aeb-8dbb-3e1608137883",
   "metadata": {},
   "source": [
    "- sparkConfは設定できる項目が少ない。ほかの設定方法を探す。\n",
    "https://spark.apache.org/docs/latest/configuration.html#environment-variables\n",
    "> Properties set directly on the SparkConf take highest precedence, then flags passed to spark-submit or spark-shell, then options in the spark-defaults.conf file.\n",
    "\n",
    "以上の記載によると、設定を反映するには\n",
    "`spark-3.2.0-bin-hadoop3.2\\conf\\spark-defaults.conf.template`\n",
    "でなく、cli上で実行するときのオプションや`spark-submit`のオプションに設定をかきｋ\n",
    "\n",
    "\n",
    "コマンドライン上で`pyspark --NAME CHANGE`\n",
    "とすると、`spark.app.name=CHANGE`になっている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ff9e4c-14ce-4761-90b5-a324101fcff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbebac5-0353-48fa-8944-14bfc8bba862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f0b639-0b76-43e7-8a1d-80c086208303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86115b7d-b41c-42f7-844f-a6fd774705c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f7edc1-2554-449c-b382-dca75058814d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 残タスク\n",
    "# 大規模な処理が行われるときに機能するか確かめる必要あり\n",
    "sc.cancelAllJobs()\n",
    "# sparkの本で確認\n",
    "https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.SparkContext.accumulator.html#pyspark.SparkContext.accumulator\n",
    "pyspark.SparkContext.accumulator¶\n",
    "# client mode と cluster mode sparkのちがい"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
